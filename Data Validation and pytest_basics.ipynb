{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Pytest basics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Main points:\n",
    "1. Store tests in a folder called tests\n",
    "2. start test function name with `test_{function_name}`\n",
    "3. use `assert` statements to check code\n",
    "4. Use `fixtures` to generate one input into many tests (e.g. data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## example\n",
    "\n",
    "import pytest\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "run = wandb.init()\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def data():\n",
    "\n",
    "    local_path = run.use_artifact(\"exercise_5/preprocessed_data.csv:latest\").file()\n",
    "    df = pd.read_csv(local_path, low_memory=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def test_data_length(data):\n",
    "    \"\"\"\n",
    "    We test that we have enough data to continue\n",
    "    \"\"\"\n",
    "    assert len(data) > 1000\n",
    "\n",
    "\n",
    "def test_number_of_columns(data):\n",
    "    \"\"\"\n",
    "    We test that we have enough data to continue\n",
    "    \"\"\"\n",
    "    assert data.shape[1] == 19"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Fixtures must have the same name of the variable in the input of the test that they are supposed to fill.\n",
    "* We can set scope to be `function` as well as `session` (other options available)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* why are starting `W&B`?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running py test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## -vv shows test file name and function, not just a .\n",
    "> pytest tests/ -vv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using `conftest.py` to add parameters and fixtures\n",
    "* we can add a file to generate shared fixtures for the tests and set-up command line parameters\n",
    "* the file sits in the `tests_` folder\n",
    "* we can add the data validation tests as a component to the pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## example of a conftest.py\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "\n",
    "run = wandb.init(project=\"exercise_9\", job_type=\"data_tests\")\n",
    "\n",
    "\n",
    "def pytest_addoption(parser):\n",
    "    parser.addoption(\"--reference_artifact\", action=\"store\")\n",
    "    parser.addoption(\"--sample_artifact\", action=\"store\")\n",
    "    parser.addoption(\"--ks_alpha\", action=\"store\")\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def data(request):\n",
    "\n",
    "    reference_artifact = request.config.option.reference_artifact\n",
    "\n",
    "    if reference_artifact is None:\n",
    "        pytest.fail(\"--reference_artifact missing on command line\")\n",
    "\n",
    "    sample_artifact = request.config.option.sample_artifact\n",
    "\n",
    "    if sample_artifact is None:\n",
    "        pytest.fail(\"--sample_artifact missing on command line\")\n",
    "\n",
    "    local_path = run.use_artifact(reference_artifact).file()\n",
    "    sample1 = pd.read_csv(local_path)\n",
    "\n",
    "    local_path = run.use_artifact(sample_artifact).file()\n",
    "    sample2 = pd.read_csv(local_path)\n",
    "\n",
    "    return sample1, sample2\n",
    "\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def ks_alpha(request):\n",
    "    ks_alpha = request.config.option.ks_alpha\n",
    "\n",
    "    if ks_alpha is None:\n",
    "        pytest.fail(\"--ks_threshold missing on command line\")\n",
    "\n",
    "    return float(ks_alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## and associated test with the fixtures passed in\n",
    "def test_kolmogorov_smirnov(data, ks_alpha):\n",
    "\n",
    "    sample1, sample2 = data\n",
    "\n",
    "    columns = [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"loudness\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "        \"duration_ms\"\n",
    "    ]\n",
    "\n",
    "    # Bonferroni correction for multiple hypothesis testing\n",
    "    # (see my blog post on this topic to see where this comes from:\n",
    "    # https://towardsdatascience.com/precision-and-recall-trade-off-and-multiple-hypothesis-testing-family-wise-error-rate-vs-false-71a85057ca2b)\n",
    "    alpha_prime = 1 - (1 - ks_alpha)**(1 / len(columns))\n",
    "\n",
    "    for col in columns:\n",
    "\n",
    "        ts, p_value = scipy.stats.ks_2samp(sample1[col], sample2[col])\n",
    "\n",
    "        # NOTE: as always, the p-value should be interpreted as the probability of\n",
    "        # obtaining a test statistic (TS) equal or more extreme that the one we got\n",
    "        # by chance, when the null hypothesis is true. If this probability is not\n",
    "        # large enough, this dataset should be looked at carefully, hence we fail\n",
    "        assert p_value > alpha_prime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deterministic and non-deterministic tests\n",
    "* see cacher and notes\n",
    "* example notes here: https://workflowy.com/s/example-t-test/EcrLBdbhULPAaiVR and https://workflowy.com/s/data-validation/S44QhMap0U8oIZtg\n",
    "* we will need to formulate what statistical test to do, what assumptions to make etc.\n",
    "* we need to be wary of multiple hypothesis testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## example - t-test for comparing means\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def test_compatible_mean(sample1, sample2):\n",
    "    \"\"\"\n",
    "    We check if the mean of the two samples is not\n",
    "    significantly different\n",
    "    \"\"\"\n",
    "    ts, p_value = scipy.stats.ttest_ind(\n",
    "        sample1, sample2, equal_var=False, alternative=\"two-sided\"\n",
    "    )\n",
    "\n",
    "    # Pre-determined threshold\n",
    "    alpha = 0.05\n",
    "\n",
    "    assert p_value >= alpha, \"T-test rejected the null hyp. at the 2 sigma level\"\n",
    "\n",
    "    return ts, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## example - ks test for comparing distributions\n",
    "def test_kolmogorov_smirnov(data):\n",
    "\n",
    "    sample1, sample2 = data\n",
    "\n",
    "    numerical_columns = [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"loudness\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "        \"duration_ms\"\n",
    "    ]\n",
    "\n",
    "    # Let's decide the Type I error probability (related to the False Positive Rate)\n",
    "    alpha = 0.05\n",
    "    # Bonferroni correction for multiple hypothesis testing\n",
    "    # (see my blog post on this topic to see where this comes from:\n",
    "    # https://towardsdatascience.com/precision-and-recall-trade-off-and-multiple-hypothesis-testing-family-wise-error-rate-vs-false-71a85057ca2b)\n",
    "    alpha_prime = 1 - (1 - alpha)**(1 / len(numerical_columns))\n",
    "\n",
    "    for col in numerical_columns:\n",
    "\n",
    "        ts, p_value = scipy.stats.ks_2samp(\n",
    "            sample1[col],\n",
    "            sample2[col],\n",
    "            alternative='two-sided'\n",
    "        )\n",
    "\n",
    "        # NOTE: as always, the p-value should be interpreted as the probability of\n",
    "        # obtaining a test statistic (TS) equal or more extreme that the one we got\n",
    "        # by chance, when the null hypothesis is true. If this probability is not\n",
    "        # large enough, this dataset should be looked at carefully, hence we fail\n",
    "        assert p_value > alpha_prime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}