{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Example of an Inference Pipeline\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Example Implementation\n",
    "* See folder `Exercise 10 - which is an implementation of an inference pipeline with one component\n",
    "* The code implements a pipeline where the data is transformed in mutiple ways"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# here is an example of a basic pipeline, with the data only processed one way\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "pipe = Pipeline(\n",
    "  steps=[\n",
    "    (\"imputer\", SimpleImputer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression())\n",
    "  ]\n",
    ")\n",
    "\n",
    "# OR\n",
    "pipe = make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# but we can actually define separate steps in the pipeline to treat different columns in different ways\n",
    "# here we define the transformers and pass them to a column transformer object\n",
    "# we then make a pipeline consisting of the transformer object and the model\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Example dataframe from the sklearn docs\n",
    "df = pd.DataFrame(\n",
    "    {'city': ['London', 'London', 'Paris', 'Sallisaw'],\n",
    "     'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n",
    "               \"A Moveable Feast\", \"The Grapes of Wrath\"],\n",
    "     'expert_rating': [5, 3, 4, 5],\n",
    "     'user_rating': [4, 5, 4, 3],\n",
    "     'click': ['yes', 'no', 'no', 'yes']})\n",
    "y = df.pop(\"click\")\n",
    "X = df\n",
    "\n",
    "# Build a Column transformer\n",
    "categorical_preproc = OneHotEncoder()\n",
    "text_preproc = TfidfVectorizer()\n",
    "numerical_preprocessing = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_transform\", categorical_preproc, ['city']),\n",
    "        (\"text_transform\", text_preproc, 'title'),\n",
    "        (\"num_transform\", numerical_preprocessing, ['expert_rating', 'user_rating'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "pipe = make_pipeline(preproc, LogisticRegression())\n",
    "pipe.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Column transformer and pipeline in example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## in the exercise 10 example, we do something similar:\n",
    "## note that this is in the run.py script in the random_forest component\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "            (\"nlp1\", nlp_transformer, nlp_features),\n",
    "        ],\n",
    "        remainder=\"drop\",  # This drops the columns that we do not transform\n",
    "    )\n",
    "\n",
    "pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(**model_config)),\n",
    "        ]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model config\n",
    "* One thing that confused me is how the model config (for the random forest) is passed in.\n",
    "* The answer: from the hydra config.yml file\n",
    "* As the config needs to go to the random_forest component, it is a little confusing!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## in the run.py script in the random_forest component, we have:\n",
    "\n",
    "# Get the configuration for the model\n",
    "    with open(args.model_config) as fp:\n",
    "        model_config = json.load(fp)\n",
    "    # Add it to the W&B configuration so the values for the hyperparams\n",
    "    # are tracked\n",
    "    wandb.config.update(model_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## and in the MLProject file for this component, we have the args.model_config ref:\n",
    "\n",
    "name: decision_tree\n",
    "conda_env: conda.yml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      train_data:\n",
    "        description: Fully-qualified name for the training data artifact\n",
    "        type: str\n",
    "      model_config:\n",
    "        description: JSON blurb containing the configuration for the decision tree\n",
    "        type: str\n",
    "    command: >-\n",
    "      python run.py --train_data {train_data} \\\n",
    "                    --model_config {model_config}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## zooming out to the main.py in the main component:\n",
    "\n",
    "## note that here we are creating a file and then writing config[\"random_forest\"] to it\n",
    "## we then use this file to pass in the model parameters for the mlflow.run which calls the component.\n",
    "## interesting that we have to do this, and can't just pass the config entries straight to the model\n",
    "\n",
    " # Serialize decision tree configuration\n",
    "    model_config = os.path.abspath(\"random_forest_config.json\")\n",
    "\n",
    "    with open(model_config, \"w+\") as fp:\n",
    "        json.dump(dict(config[\"random_forest\"]), fp)\n",
    "\n",
    "    _ = mlflow.run(\n",
    "        os.path.join(root_path, \"random_forest\"),\n",
    "        \"main\",\n",
    "        parameters={\n",
    "            \"train_data\": config[\"data\"][\"train_data\"],\n",
    "            \"model_config\": model_config\n",
    "        },\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## for reference, here is the config.yml file:\n",
    "\n",
    "main:\n",
    "  project_name: exercise_10\n",
    "  experiment_name: dev\n",
    "data:\n",
    "  train_data: \"exercise_6/data_train.csv:latest\"\n",
    "random_forest:\n",
    "  n_estimators: 100\n",
    "  criterion: 'gini'\n",
    "  max_depth: null\n",
    "  min_samples_split: 2\n",
    "  min_samples_leaf: 1\n",
    "  min_weight_fraction_leaf: 0.0\n",
    "  max_features: 'auto'\n",
    "  max_leaf_nodes: null\n",
    "  min_impurity_decrease: 0.0\n",
    "  min_impurity_split: null\n",
    "  bootstrap: true\n",
    "  oob_score: false\n",
    "  n_jobs: null\n",
    "  random_state: null\n",
    "  verbose: 0\n",
    "  warm_start: false\n",
    "  class_weight: null\n",
    "  ccp_alpha: 0.0\n",
    "  max_samples: null"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline implementation in PyTorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## example\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import Sequential, Softmax\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Get a pre-trained model\n",
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the inference pipeline\n",
    "pipe = Sequential(\n",
    "    # NOTE: for the pipeline to be scriptable with script,\n",
    "    # you must use a list [256, 256] instead of just one number (256)\n",
    "    transforms.Resize([256, 256]),\n",
    "    transforms.CenterCrop([224, 224]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    model,\n",
    "    Softmax(1)\n",
    ")\n",
    "\n",
    "# Save inference artifact using torch.script\n",
    "scripted = torch.jit.script(pipe)\n",
    "scripted.save(\"inference_artifact.pt\")\n",
    "\n",
    "# NOTE: normally we would upload it to the artifact store\n",
    "\n",
    "# Load inference artifact\n",
    "pipe_reload = torch.jit.load(\"inference_artifact.pt\")\n",
    "\n",
    "# Load one example\n",
    "# NOTE: these operations are usually taken care by the inference\n",
    "# engine\n",
    "img = Image.open(\"dog.jpg\")\n",
    "img.load()\n",
    "# Make into a batch of 1 element\n",
    "data = transforms.ToTensor()(np.asarray(img, dtype=\"uint8\").copy()).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    logits = pipe_reload(data).detach()\n",
    "\n",
    "proba = logits[0]\n",
    "\n",
    "# Transform to class and print answer\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    classes = [s.strip() for s in f.readlines()]\n",
    "print(f\"Classification: {classes[proba.argmax()]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}