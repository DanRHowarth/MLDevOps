{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. EDA examples from the course"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# example of how to execute pandas profiling\n",
    "import pandas_profiling\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"genres_mod.parquet\")\n",
    "profile = pandas_profiling.ProfileReport(df)\n",
    "profile.to_widgets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Data Segregation\n",
    "* use of a temp file\n",
    "    * looks like we have to create a file locally and then add it to W&B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)-15s %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def go(args):\n",
    "\n",
    "    run = wandb.init(project=\"exercise_6\", job_type=\"split_data\")\n",
    "\n",
    "    logger.info(\"Downloading and reading artifact\")\n",
    "    artifact = run.use_artifact(args.input_artifact)\n",
    "    artifact_path = artifact.file()\n",
    "\n",
    "    df = pd.read_csv(artifact_path, low_memory=False)\n",
    "\n",
    "    # Split first in model_dev/test, then we further divide model_dev in train and validation\n",
    "    logger.info(\"Splitting data into train, val and test\")\n",
    "    splits = {}\n",
    "\n",
    "    splits[\"train\"], splits[\"test\"] = train_test_split(\n",
    "        df,\n",
    "        test_size=args.test_size,\n",
    "        random_state=args.random_state,\n",
    "        stratify=df[args.stratify] if args.stratify != 'null' else None,\n",
    "    )\n",
    "\n",
    "    # Save the artifacts. We use a temporary directory so we do not leave\n",
    "    # any trace behind\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "\n",
    "        for split, df in splits.items():\n",
    "\n",
    "            # Make the artifact name from the provided root plus the name of the split\n",
    "            artifact_name = f\"{args.artifact_root}_{split}.csv\"\n",
    "\n",
    "            # Get the path on disk within the temp directory\n",
    "            temp_path = os.path.join(tmp_dir, artifact_name)\n",
    "\n",
    "            logger.info(f\"Uploading the {split} dataset to {artifact_name}\")\n",
    "\n",
    "            # Save then upload to W&B\n",
    "            df.to_csv(temp_path)\n",
    "\n",
    "            artifact = wandb.Artifact(\n",
    "                name=artifact_name,\n",
    "                type=args.artifact_type,\n",
    "                description=f\"{split} split of dataset {args.input_artifact}\",\n",
    "            )\n",
    "            artifact.add_file(temp_path)\n",
    "\n",
    "            logger.info(\"Logging artifact\")\n",
    "            run.log_artifact(artifact)\n",
    "\n",
    "            # This waits for the artifact to be uploaded to W&B. If you\n",
    "            # do not add this, the temp directory might be removed before\n",
    "            # W&B had a chance to upload the datasets, and the upload\n",
    "            # might fail\n",
    "            artifact.wait()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Split a dataset into train and test\",\n",
    "        fromfile_prefix_chars=\"@\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--input_artifact\",\n",
    "        type=str,\n",
    "        help=\"Fully-qualified name for the input artifact\",\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--artifact_root\",\n",
    "        type=str,\n",
    "        help=\"Root for the names of the produced artifacts. The script will produce 2 artifacts: \"\n",
    "             \"{root}_train.csv and {root}_test.csv\",\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--artifact_type\", type=str, help=\"Type for the produced artifacts\", required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--test_size\",\n",
    "        help=\"Fraction of dataset or number of items to include in the test split\",\n",
    "        type=float,\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--random_state\",\n",
    "        help=\"An integer number to use to init the random number generator. It ensures repeatibility in the\"\n",
    "             \"splitting\",\n",
    "        type=int,\n",
    "        required=False,\n",
    "        default=42\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--stratify\",\n",
    "        help=\"If set, it is the name of a column to use for stratified splitting\",\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='null'  # unfortunately mlflow does not support well optional parameters\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    go(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# which is run\n",
    "\n",
    "python run.py --input_artifact {input_artifact} \\\n",
    "                    --artifact_root {artifact_root} \\\n",
    "                    --artifact_type {artifact_type} \\\n",
    "                    --test_size {test_size} \\\n",
    "                    --random_state {random_state} \\\n",
    "                    --stratify {stratify}\n",
    "\n",
    "# or,\n",
    "\n",
    "mlflow run . [parameters]\n",
    "\n",
    "# with mlflow configured"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}